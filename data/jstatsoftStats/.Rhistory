html_parsed <- read_html(url, encoding = "UTF-8")
# step 2: construct and apply XPath expression using SelectorGadget (in browser)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]'
headings <- html_nodes(html_parsed, xpath = xpath) %>% html_text()
headings
# step 3: clean data
headings <- str_replace_all(headings, "\\n" , " ") %>% str_trim()
headings
# step 4: plot data in word cloud
headings_dfm <- dfm(headings, remove = stopwords(), remove_punct = TRUE)
textplot_wordcloud(headings_dfm)
warnings()
textplot_wordcloud(headings_dfm)
## load packages
library(rvest)
library(stringr)
library(quanteda)
# step 1: parse page
url <- "http://www.nytimes.com"
## load packages
library(rvest)
library(stringr)
library(quanteda)
# step 1: parse page
url <- "http://www.nytimes.com"
html_parsed <- read_html(url, encoding = "UTF-8")
# step 2: construct and apply XPath expression using SelectorGadget (in browser)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
headings <- html_nodes(html_parsed, xpath = xpath) %>% html_text()
headings
# step 1: parse page
url <- "http://www.nytimes.com"
html_parsed <- read_html(url, encoding = "UTF-8")
# step 2: construct and apply XPath expression using SelectorGadget (in browser)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
headings <- html_nodes(html_parsed, xpath = xpath) %>% html_text()
headings
## load packages
library(rvest)
library(stringr)
library(quanteda)
# step 1: parse page
url <- "http://www.nytimes.com"
html_parsed <- read_html(url, encoding = "UTF-8")
# step 2: construct and apply XPath expression using SelectorGadget (in browser)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
headings <- html_nodes(html_parsed, xpath = xpath) %>% html_text()
headings
# step 3: clean data
headings <- str_replace_all(headings, "\\n" , " ") %>% str_trim()
headings
# step 4: plot data in word cloud
headings_dfm <- dfm(headings, remove = stopwords(), remove_punct = TRUE)
textplot_wordcloud(headings_dfm)
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# load RSelenium
library(RSelenium)
library(rvest)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
titledatElem1$clickElement() # click on list element
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# install current version of Java SE Development Kit
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html")
# load RSelenium
library(RSelenium)
library(rvest)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
titledatElem1$clickElement() # click on list element
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
source("packages.r")
# to create strings, both single and double quotes work
string1 <- 'This is a string'
string2 <- 'If I want to include a "quote" inside a string, I use single quotes'
# to include a literal single or double quote in a string you can use \ to escape it:
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
double_quote
single_quote
# printed representation of a string shows the escapes
double_quote
writeLines(double_quote) # shows raw contents of the string
cat(double_quote)
?cat
# not very consistent when it comes to UTF-8 code points
x <- "\u00b5"
x
writeLines(x)
# inspect text
x <- c("apple", "banana", "pear")
str_view(x, "an")
str_view_all(x, "an")
str_view_all(x, "a")
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:punct:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
# for more character classes, see
?base::regex
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
# quantifier
str_extract(example.obj, "s[[:alpha:]][[:alpha:]][[:alpha:]]l")
# for more character classes, see
?base::regex
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# greedy quantification
str_extract(example.obj, "A.+sentence")
str_extract(example.obj, "A.+?sentence")
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
example.obj
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# joining
str_c("text", "manipulation", sep = " ")
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
library(rvest)
library(stringr)
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
buildings <- tables[[5]]
names(buildings)
head(buildings)
table(buildings$`Country`) %>% sort(decreasing = TRUE)  %>% .[1:5]
table(buildings$City) %>% sort(decreasing = TRUE)  %>% .[1:5]
buildings$height <- str_extract(buildings$`Planned architectural height`, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric()
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
buildings <- tables[[6]]
names(buildings)
head(buildings)
table(buildings$`Country`) %>% sort(decreasing = TRUE)  %>% .[1:5]
table(buildings$City) %>% sort(decreasing = TRUE)  %>% .[1:5]
buildings$height <- str_extract(buildings$`Planned architectural height`, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric()
sum(buildings$height)
parsed_doc <- read_html("https://google.com")
library(rvest)
parsed_doc <- read_html("https://google.com")
parsed_doc
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
as_list(parsed_doc)
html_structure(parsed_doc)
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
html_nodes(parsed_doc, xpath = "//title/..")
html_nodes(parsed_doc, xpath = "//address | //title")
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
html_nodes(parsed_doc, xpath = "//title/parent::*")
html_nodes(parsed_doc, xpath = "//div[not(count(./@*)>2)]")
html_nodes(parsed_doc, xpath = "//title")
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
url <- "https://www.nytimes.com"
url_parsed <- read_html(url)
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
headings_nodes
headings <- html_text(headings_nodes)
headings
headings <- str_replace_all(headings, "\\n|\\t|\\r", "") %>% str_trim()
library(stringr)
headings <- str_replace_all(headings, "\\n|\\t|\\r", "") %>% str_trim()
head(headings)
headings
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_human_spaceflights")
?html_table
tables <- html_table(url_p, header = TRUE, fill = TRUE)
html_table
tables[[1]]
tables[[2]]
tables[[3]]
spaceflights <- tables[[1]]
spaceflights
class(spaceflights)
names(spaceflights)
spaceflights$`Russia Soviet Union`
spaceflights$`RussiaÂ Soviet Union`
browseURL("http://selectorgadget.com/")
url <- "https://www.nytimes.com"
xpath <-  '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = xpath) %>% html_text()
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/rscraping-hertie-2018")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
url_out
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
library(rvest)
library(httr)
library(stringr)
url_parsed <- read_html("http://www.google.com")
html_form(url_parsed)
search <- html_form(url_parsed)[[1]]
search
form <- set_values(search, q = "kneipen kreuzberg")
form
google_search <- submit_form(url_parsed, form)
google_search <- submit_form("http://www.google.com", form)
google_search <- submit_form(search, form)
form
?submit_form
session <- html_session("http://www.google.com")
session
google_search <- submit_form(session, form)
?submit_form
google_search
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_text
hits_links
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet <- html_form(url_parsed)[[1]]
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
url <- "http://read-able.com/"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
readable_form
url <- "http://read-able.com/"
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
browseURL("http://www.jstatsoft.org/")
session <- html_session("http://www.google.com")
library(rvest)
library(httr)
library(stringr)
session <- html_session("http://www.google.com")
session
search <- html_form(session)[[1]]
html_form(session)
search <- html_form(session)[[1]]
form <- set_values(search, q = "kneipen kreuzberg")
form
google_search <- submit_form(session, form)
?submit_form
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_text
hits_links
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
url <- "http://wordnetweb.princeton.edu/perl/webwn"
url_parsed <- read_html(url)
html_form(url_parsed)
wordnet <- html_form(url_parsed)[[1]]
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data", o2 = "1")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
url <- "http://read-able.com/"
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
readable_form
session <- html_ses sion(url, user_agent(uastring))
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
?dir.create
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,83,1))
volurl
volurl[1:9] <- paste0("00", seq(1, 9, 1))
volurl
issurl <- paste0("0", seq(1,9,1))
issurl
urls_list <- paste0(baseurl, volurl)
urls_list
urls_list <- paste0(rep(urls_list, each = 9), "i", issurl)
urls_list
747*20
names <- paste0(rep(volurl, each = 9), "_", issurl, ".html")
names
# download pages
folder <- "html_articles/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, names[i])) # , method = "libcurl" might be needed on windows machine
Sys.sleep(runif(1, 0, 1))
}
}
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
files_size <- sapply(list_files_path, file.size)
table(files_size) %>% sort()
library(rvest)
library(stringr)
library(magrittr)
table(files_size) %>% sort()
delete_files <- list_files_path[files_size == 27131]
delete_files
sapply(delete_files, file.remove)
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# delete non-existing articles
files_size <- sapply(list_files_path, file.size)
table(files_size) %>% sort()
delete_files <- list_files_path[files_size == 27131]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
authors
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish, stringsAsFactors = FALSE)
View(dat)
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
plot(density(dat$numViews, from = 0), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JStatSoft")
summary(dat$numViews)
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
