NEWSAPI_KEY <- "dd893f503fc24af2b4a47889244eaf27"
## save to .Renviron file
cat(
paste0("NEWSAPI_KEY=", NEWSAPI_KEY),
append = TRUE,
fill = TRUE,
file = file.path("~", ".Renviron")
)
src <- get_sources(language = "en")
src
src <- get_sources(language = "de")
src
df <- lapply(src$id, get_articles)
?get_articles
src <- get_sources(language = "en")
df <- lapply(src$id, get_articles)
get_articles("espn")
get_articles
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df
df <- do.call("rbind", df)
View(df)
src <- get_sources(language = "de")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "latest")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "top")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
get_articles("bild", apiKey = NEWSAPI_KEY)
source("packages.r")
source("functions.r")
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
?html_session
browseURL("https://www.google.com/robots.txt")
browseURL("http://www.nytimes.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
browseURL("http://httpbin.org")
source("packages.r")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(Hello = "my@email.com"))
browseURL("https://www.google.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "http://facebook.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
?paths_allowed
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
source("packages.r")
source("functions.r")
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
browseURL("http://ip-api.com/")
library(ipapi)
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
system("java -version")
rD <- rsDriver()
source("packages.r")
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
library(rtweet)
install.packages("rtweet")
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
library(rtweet)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
appname <- "TwitterToR"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
install.packages("httpuv")
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
q <- paste0("schulz,merkel,btw17,btw2017")
?stream_tweets
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
class(twitter_stream_ger)
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
names(rt)
head(rt)
View(rt)
users_data(rt) %>% head()
users_data(rt) %>% names()
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
60*60
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
language = "de",
token = twitter_token)
source("packages.r")
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
1500*0.5*.5*5
1500*.5*.5*5
30/80
30/66
1700*.6*.5*5
1700*.15*.5*5
library(rtweet)
appname <- "TwitterToR"
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
?search_tweets
ts_plot(rt, by = "mins", theme = "spacegray",
main = "Tweets about Trump")
rt <- search_tweets("merkel", n = 2000, include_rts = TRUE, token = twitter_token)
names(rt)
View(rt)
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
ts_plot(rt, by = "mins", theme = "spacegray",
main = "Tweets about Merkel")
rt$created_at
?ts_plot
ts_plot(rt, by = "mins", theme = "spacegray", main = "Tweets about Merkel")
ts_plot(rt, by = "days", theme = "spacegray", main = "Tweets about Merkel")
ts_plot(rt, by = "hours", theme = "spacegray", main = "Tweets about Merkel")
?search_tweets
rt <- search_tweets("merkel :(", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
rt <- search_tweets("tauber :(", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
rt <- search_tweets(URLEncode("tauber :("), n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
rt <- search_tweets(URLencode("tauber :("), n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt <- search_tweets(URLencode("tauber :("), n = 1000, include_rts = FALSE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
tauber_good <- search_tweets(URLencode("tauber :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
search_tweets
.search_tweets
getAnywhere(".search_tweets")
?make_url
getAnywhere("make_url")
tauber_good <- search_tweets(URLencode("tauber filter:images"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good
tauber_good$text
source("packages.r")
source("functions.r")
browseURL("http://ip-api.com/docs/")
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json/72.33.67.89"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
browseURL("http://ip-api.com/docs/")
source("packages.r")
source("functions.r")
ip_parsed <- xml2::read_xml(url)
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
?as_list
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
class(ip_parsed)
ip_parsed %>% unlist
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
source("packages.r")
source("functions.r")
browseURL("https://dev.twitter.com/rest/public/search")
appname <- "TwitterToR"
load("/Users/munzerts/rkeys.RDa")
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
?search_tweets
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
dim(rt)
names(rt)
View(rt)
URLencode("tauber :(")
tauber_bad <- search_tweets(URLencode("tauber :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_bad$text
tauber_good <- search_tweets(URLencode("tauber :)"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
tauber_good <- search_tweets(URLencode("tauber filter:images"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
ts_plot(rt, by = "hours", theme = "spacegray", main = "Tweets about Merkel")
q <- paste0("schulz,merkel,btw17,btw2017")
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
View(twitter_stream_ger)
rt <- parse_stream("data/rtweet-data/btw17_2017-07-03-13-02-52.json")
merkel <- str_detect(rt$text, regex("merkel", ignore_case = TRUE))
schulz <- str_detect(rt$text, regex("schulz", ignore_case = TRUE))
mentions_df <- data.frame(merkel,schulz)
colMeans(mentions_df, na.rm = TRUE)
lookup_users
user_df <- lookup_users("RDataCollection")
names(user_df)
user_timeline_df <- get_timeline("RDataCollection")
names(user_timeline_df)
user_timeline_df$text
user_favorites_df <- get_favorites("RDataCollection")
browseURL("http://pablobarbera.com/big-data-upf/")
157.77+22.95+22.95+2.4
36.8+25.9+8.9+8.1+8+8+4.3
source("packages.r")
source("functions.r")
library("WikipediR")
library("WikidataR")
library("pageviews")
library("statsgrokse")
links <- page_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
unlist(links) %>% as.character()
?str_subset
unlist(links) %>% as.character() %>% str_subset("[^0]")
str(wp_content)
wp_content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor")
content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor")
str(content)
content$text
content$parse$text
content$parse$text %>% class
?page_content
content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor", as_wikitext = TRUE)
str(content)
content$parse$text
content$parse$wikitext
?page_backlinks
backlinks <- page_backlinkslinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
backlinks
unlist(backlinks) %>% as.character() %>% str_subset("[^0]")
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
unlist(backlinks) %>% as.character() %>% str_subset("[^0]")
links <- page_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
unlist(links) %>% as.character() %>% str_subset("[^0]")
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
backlinks
unlist(backlinks)
unlist(backlinks) %>% names()
backlinks[names(backlinks) == "title"]
unlist(backlinks) %>% as.character() %>% .[names(.) == "title"]
unlist(backlinks) %>%  .[names(.) == "title"]
unlist(backlinks) %>%  .[names(.) == "title"] %>% as.character
?page_external_links
externallinks <- page_external_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
externallinks <- page_external_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500)
externallinks
?page_info
pageinfo <- page_info("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
pageinfo <- page_info("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE)
pageinfo
str(pageinfo)
categories_in_page("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE)
?find_item
?find_item
wd_item <- find_item("Brandenburger Tor", language = "de")
wd_item
get_item("82425")
tor_item <- get_item("82425")
wd_item
str(wd_item)
class(v)
class(wd_item)
wd_item <- find_item("Brandenburger Tor", language = "de")
class(wd_item)
str(wd_item)
wd_item[[ยก]]
wd_item[[1
wd_item[[1]]$id
tor_item <- get_item("82425")
tor_item
extract_claims(tor_item)
?extract_claims
tor_item
str(tor_item)
names(tor_item)
tor_item$items
tor_item$claims
str(tor_item)
unlist(tor_item)
str(v)
str(tor_item)
tor_item[[1]]
str(tor_item)
unlist(tor_item) %>% names
names_item <- unlist(tor_item) %>% names
tor_item$type
tor_item$id
tor_item$labels
str(tor_item)
names_item <- unlist(tor_item) %>% names
names_item
extract_claims(tor_item, "P31")
extract_claims(tor_item, "P17")
extract_claims(tor_item, "P17")
get_property(tor_item, "P17")
?get_property
get_property(tor_item, "P40")
tor_item <- get_item("82425")
get_property("P40")
get_property("P17")
extract_claims(tor_item, "P17")
get_property("Q183")
extract_claims(tor_item, "P17")
get_property("Q183")
property <- get_property("Q183")
str(property)
property <- get_property("Q183")
property
# search Wikipedia with search term
searchWikiFun <- function(term = NULL, limit = 100, title.only = TRUE, wordcount.min = 500) {
# API doc at https://www.mediawiki.org/wiki/API:Search
term <- URLencode(term)
url <- sprintf("https://de.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&srlimit=%d&format=json", term, limit)
wiki_search_parsed <- jsonlite::fromJSON(url)$query$search
wiki_search_parsed <- dplyr::filter(wiki_search_parsed, wordcount >= wordcount.min)
if(title.only == FALSE) {
return(wiki_search_parsed)
} else{
return(wiki_search_parsed$title)
}
}
# search for categories and pages within these categories
searchWikiCatsFun <- function(pages = NULL, categories = NULL, language = "de", project = "wikipedia", limit = 100, output = c("pages", "cats"), subcats = FALSE, max.numcats = 5) {
if(!is.null(pages)) {
# search categories
cats <- llply(pages, function(x) { try(categories_in_page(language, project, pages = x, clean_response = TRUE, limit = limit)[[1]]$categories$title, silent = TRUE)   }) %>% unlist %>% table %>% sort(decreasing = TRUE) %>% extract(1:max.numcats) %>% names() %>% extract(!str_detect(., "^Kategorie:Wikipedia:Lesenswert")) %>% unique %>% extract(str_detect(., "^Kategorie")) %>% str_replace("Kategorie:", "")
# identify subcategories in categories
if(subcats == TRUE){
subcats <- llply(cats, function(x) { try(pages_in_category(language, project, categories = x, type = "subcat", limit = limit, clean_response = TRUE)$title, silent = TRUE)   }) %>% unlist %>% unique
subcats <- subcats[str_detect(subcats, "^Kategorie")] %>% str_replace("Kategorie:", "")
# combine categories
cats_all <- c(cats, subcats) %>% unique
}else{
cats_all <- cats
}
}else{
cats_all <- categories
}
# find pages in all categories
pages_in_cats <- llply(cats_all, function(x) { try(pages_in_category(language, project, categories = x, type = "page", limit = limit, clean_response = TRUE)$title, silent = TRUE)   }) %>% unlist %>% unique
# return output
if(output == "pages") {
return(pages_in_cats)
}else{
return(cats_all)
}
}
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
pages_search
(cats_search <- searchWikiCatsFun(pages = pages_search, output = "cats", subcats = TRUE, max.numcats = 10))
# quick assessment of pages' recent pageview statistics
pagesMinPageviews <- function(pages = NULL, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50) {
pageviews_list <- list()
pageviews_mean <- numeric()
for (i in seq_along(pages)) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
}
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = num(pageviews_mean), stringsAsFactors = FALSE)
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
return(pages_minviews)
}
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2008-01-01", to = "2013-12-31", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(wp_trend(URLencode(pages[i]), from = from, to = to, lang = language))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50)
pagenames_arbeitslosigkeit
dir.create("data/wikipageviews")
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
source("packages.r")
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
library(statsgrokse)
?statsgrokse
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2008-01-01", to = "2013-12-31", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(statsgrokse(URLencode(pages[i]), from = from, to = to, lang = language))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2015070100", to = "2017040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2015070100", to = "2017040100", language = "de")
source("packages.r")
tempwd <- ("data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
